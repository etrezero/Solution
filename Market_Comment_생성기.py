# ìš´ìš©ë³´ê³ ì„œ ìë™í™” ì½”ë©˜íŠ¸ ìƒì„±ê¸° (Dash UI í†µí•©)

# í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬
import os
import sys
import re
import json
import yaml
import base64
import pickle
import logging
from pathlib import Path
from datetime import datetime, timedelta
from collections import Counter
from dateutil.relativedelta import relativedelta

# ì™¸ë¶€ íŒ¨í‚¤ì§€
import pandas as pd
import numpy as np
from pandas.tseries.offsets import MonthEnd
from openpyxl import Workbook
from sklearn.feature_extraction.text import CountVectorizer

import fitz  # PyMuPDF
import pymysql
import requests
import concurrent.futures
import win32com.client

# ê¸ˆìœµ ë°ì´í„°
import yfinance as yf
from pykrx import stock as pykrx

# Dash ê´€ë ¨
import dash
from dash import html, dcc, Input, Output, State, dash_table
import dash_bootstrap_components as dbc

# Plotly ì‹œê°í™”
import plotly.graph_objs as go
import plotly.express as px

# ì‚¬ìš©ì ì •ì˜ í•¨ìˆ˜
from get_hostname_from_ip import get_hostname_from_ip
import pythoncom





# ê¸°ë³¸ CONFIG ê²½ë¡œ ì„¤ì •
DEFAULT_CONFIG_PATH = r"C:\Covenant\config.yaml"
LOCAL_CONFIG_PATH = r"C:\Covenant\config.yaml"

# í˜¸ìŠ¤íŠ¸ì—ì„œ API í‚¤ ë¶ˆëŸ¬ì˜¤ê¸° (ì„œë²„ ë‚´ë¶€ìš©)
def load_openai_key():
    with open(LOCAL_CONFIG_PATH, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)["openai_api_key"]

def load_news_api_key():
    with open(LOCAL_CONFIG_PATH, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)["newsapi"]







cache_price = r'C:\Covenant\cache\Market_Comment_price.pkl'
cache_expiry = timedelta(days=1)



# ì£¼ì‹ ì½”ë“œ ëª©ë¡ ì„¤ì •
code_dict = {
    'ACWI': 'ê¸€ë¡œë²Œì£¼ì‹',
    'ACWX': 'ë¯¸êµ­ì™¸ ê¸€ë¡œë²Œì£¼ì‹',
    'BND': 'ê¸€ë¡œë²Œì±„ê¶Œ',
    'DIA': 'ë¯¸êµ­ì£¼ì‹',
    'VUG': 'ë¯¸êµ­ì„±ì¥ì£¼',
    'VTV': 'ë¯¸êµ­ê°€ì¹˜ì£¼',
    'VEA': 'ì„ ì§„êµ­ì£¼ì‹',
    'VWO': 'ì‹ í¥êµ­ì£¼ì‹',
    'MCHI': 'MSCIì¤‘êµ­ì£¼ì‹',
    'HYG': 'ë¯¸êµ­í•˜ì´ì¼ë“œì±„ê¶Œ',
    'GLD': 'ê¸ˆ',
    'KRW=X': 'ì›í™”í™˜ìœ¨',
    '356540.KS': 'í•œêµ­ì£¼ì‹',

  # ğŸ”½ ì¶”ê°€ëœ ì„¹í„° ETF
    'XLF': 'ë¯¸êµ­ê¸ˆìœµì„¹í„°',
    'IYF': 'ë¯¸êµ­ê¸ˆìœµì§€ì£¼',
    'XLK': 'ë¯¸êµ­ê¸°ìˆ ì„¹í„°',
    'XLY': 'ë¯¸êµ­ì†Œë¹„ì¬ì„¹í„°',
    'XLE': 'ë¯¸êµ­ì—ë„ˆì§€ì„¹í„°',
    'XLV': 'ë¯¸êµ­í—¬ìŠ¤ì¼€ì–´ì„¹í„°',
    'XLI': 'ë¯¸êµ­ì‚°ì—…ì„¹í„°',
    'XLP': 'ë¯¸êµ­í•„ìˆ˜ì†Œë¹„ì¬ì„¹í„°',
    'XLU': 'ë¯¸êµ­ìœ í‹¸ë¦¬í‹°ì„¹í„°',
    'XLC': 'ë¯¸êµ­ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ì„¹í„°',
    'XLB': 'ë¯¸êµ­ì†Œì¬ì„¹í„°',

}

code = list(code_dict.keys())  # ì£¼ì‹ ì½”ë“œ ëª©ë¡


#? pip install curl_cffi
# ë°ì´í„° ê°€ì ¸ì˜¤ê¸° í•¨ìˆ˜
def fetch_data(code, start, end):
    from curl_cffi import requests

    try:
        if isinstance(code, int) or code.isdigit():
            if len(code) == 5:
                code = '0' + code
            df_price = pykrx.get_market_ohlcv_by_date(start, end, code)['ì¢…ê°€']
        else:
            import urllib3
            urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

            session = requests.Session(impersonate="chrome")
            session.verify = False  # SSL ì¸ì¦ì„œ ê²€ì¦ ë¹„í™œì„±í™”
            yf_data = yf.Ticker(code, session=session)
            df_price = yf_data.history(start=start, end=end)['Close']
            df_price = df_price.tz_localize(None)  # íƒ€ì„ì¡´ ì œê±°

        df_price = pd.DataFrame(df_price)
        df_price.columns = [code]
        df_price.index = pd.to_datetime(df_price.index).strftime('%Y-%m-%d')  # ì¸ë±ìŠ¤ë¥¼ ë¬¸ìì—´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        df_price = df_price.sort_index(ascending=True)
        
        return df_price
    
    except Exception as e:
        print(f"Error fetching data for {code}: {e}")
        return None


# ìºì‹œ ë°ì´í„° ë¡œë”© ë° ë°ì´í„° ë³‘í•© ì²˜ë¦¬ í•¨ìˆ˜
def Func(code, start, end, batch_size=10):
    if os.path.exists(cache_price):
        cache_mtime = datetime.fromtimestamp(os.path.getmtime(cache_price))
        if datetime.now() - cache_mtime < cache_expiry:
            with open(cache_price, 'rb') as f:
                print("Loading data from cache...")
                return pickle.load(f)

    data_frames = []
    for i in range(0, len(code), batch_size):
        code_batch = code[i:i + batch_size]
        with concurrent.futures.ThreadPoolExecutor() as executor:
            futures = {executor.submit(fetch_data, c, start, end): c for c in code_batch}
            for future in concurrent.futures.as_completed(futures):
                result = future.result()
                if result is not None:
                    data_frames.append(result)

    price_data = pd.concat(data_frames, axis=1) if data_frames else pd.DataFrame()
    price_data = price_data.sort_index(ascending=True)
    print("price_data=================\n", price_data)

    with open(cache_price, 'wb') as f:
        pickle.dump(price_data, f)
        print("Data cached.")

    return price_data




# ì˜¤ëŠ˜ ë‚ ì§œì™€ ê¸°ê°„ ì„¤ì •

from pandas.tseries.offsets import MonthEnd

# ì˜¤ëŠ˜ ê¸°ì¤€ ì§€ì§€ë‚œë‹¬ ë§, ì§€ë‚œë‹¬ ë§ ê³„ì‚°
today = datetime.today()
last_month_end = (today - MonthEnd(1)).date()      # ì§€ë‚œë‹¬ ë§
prev_month_end = (today - MonthEnd(2)).date()      # ì§€ì§€ë‚œë‹¬ ë§


df_price = Func(code, prev_month_end, last_month_end, batch_size=10)
df_price = df_price.ffill()





# ì¸ë±ìŠ¤ë¥¼ datetimeìœ¼ë¡œ ë³€í™˜ (ë¬¸ìì—´ì´ë©´)
df_price.index = pd.to_datetime(df_price.index)
df_price.columns = [code_dict.get(col, col) for col in df_price.columns]
print("df_price===============", df_price)


# ì§€ì§€ë‚œë‹¬ ë§ê³¼ ì§€ë‚œë‹¬ ë§ì˜ ë°ì´í„°ë§Œ í•„í„°ë§
df_month_ends = df_price.loc[df_price.index.isin([prev_month_end, last_month_end])]

# ìˆ˜ìµë¥  ê³„ì‚°
ì›”ê°„ìˆ˜ìµë¥  = df_month_ends.pct_change().iloc[-1:].copy()  # ë§ˆì§€ë§‰ í–‰ë§Œ (ì§€ë‚œë‹¬ë§ ê¸°ì¤€ ìˆ˜ìµë¥ )
ì›”ê°„ìˆ˜ìµë¥ .index = [last_month_end]  # ì¸ë±ìŠ¤ë¥¼ ë‚ ì§œë¡œ ì„¤ì •

ì›”ê°„ìˆ˜ìµë¥  = (ì›”ê°„ìˆ˜ìµë¥  * 100).round(1).astype(str) + '%'

# ê²°ê³¼ ì¶œë ¥
print("ğŸ“ˆ ì›”ê°„ ìˆ˜ìµë¥  ====================", ì›”ê°„ìˆ˜ìµë¥ )







# Dash ì•± ìƒì„±
app = dash.Dash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])
app.title = "Market Comment"


# ë ˆì´ì•„ì›ƒ ì •ì˜
app.layout = dbc.Container([
    html.H3("Market Comment ìƒì„±ê¸°", className="text-center my-4"),

    dbc.Row([
        dbc.Col([
            dbc.Card([
                dbc.CardBody([
                    html.Label("âœ… config.yaml ê²½ë¡œ", className="fw-bold mb-1"),
                    dcc.Input(id='config-path', value=DEFAULT_CONFIG_PATH, type='text', style={"width": "100%"}),

                    html.Br(), html.Br(),

                    html.Label("ğŸ“ ëª…ë ¹ì–´ (ì˜ˆ: ì‹œì¥í˜„í™©ê³¼ ì‹œì¥ì „ë§ ì‘ì„±í•´ì¤˜)", className="fw-bold mb-1"),
                    dcc.Textarea(
                        id='nl-command', 
                        value=f"ì‹œì¥í˜„í™©ê³¼ ì‹œì¥ì „ë§ ì‘ì„±í•´ì¤˜. newsì™€ PDFì˜ ë‚´ìš©ì€ ê²½ì œì™€ ìì‚°êµ°ì— ëŒ€í•œ ì„¤ëª…ì— ë³´ì¡°ì  ì—­í• ì„ í•˜ëŠ” ìˆ˜ì¤€ìœ¼ë¡œë§Œ í™œìš©", 
                        style={'width': '100%', 'height': 100}),

                    html.Br(),
                    dbc.Button("ì½”ë©˜íŠ¸ ìƒì„± ì‹¤í–‰", id='run-button', color='primary', className='w-100')
                ])
            ])
        ], width=4, style={"height": "300vh", "overflowY": "auto"}),

        dbc.Col([
            dbc.Card([
                dbc.CardHeader("Market Comment", className="fw-bold"),
                dbc.CardBody([
                    dcc.Loading(
                        html.Div(id='reply-output', style={'whiteSpace': 'pre-wrap', 'minHeight': '400px'}),
                        type='circle'
                    ),

                html.Br(),
                    dbc.Button("ğŸ“‹ ì½”ë©˜íŠ¸ ë³µì‚¬", id="copy-button", color="secondary", className="mt-2"),
                    
                    html.Div(id="dummy-output", style={"display": "none"}),  # ì½œë°± ê²°ê³¼ë¥¼ ë°›ëŠ” ë”ë¯¸ Div

                

                ])
            ])
        ], width=7, style={"height": "200vh", "overflowY": "auto"}),



        
    ])
], fluid=True, style={"maxWidth": "70%", "margin": "0 auto"})




# ì˜¤ë¥˜ ë©”ì‹œì§€ íŒŒì‹± í•¨ìˆ˜
def extract_missing_variable(error_msg):
    match = re.search(r"NameError: name '(\w+)' is not defined", error_msg)
    return match.group(1) if match and match.group(1) not in ['fig', 'df', 'table', 'text'] else None




#? <PDF ìš”ì•½>============================================
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
PDF_DIR = Path("assets/Outlook_PDFs")
PDF_DIR.mkdir(parents=True, exist_ok=True)

# Outlookì—ì„œ PDF ì €ì¥
def fetch_recent_pdf_attachments(days=30, save_dir=PDF_DIR):
    pythoncom.CoInitialize()  # âœ… COM ì´ˆê¸°í™”
    outlook = win32com.client.Dispatch("Outlook.Application").GetNamespace("MAPI")
    inbox = outlook.GetDefaultFolder(6)
    messages = inbox.Items
    messages.Sort("[ReceivedTime]", True)
    recent_date = datetime.now() - timedelta(days=days)
    saved_files = []
    for message in messages:
        try:
            if isinstance(message.ReceivedTime, datetime) and message.ReceivedTime.replace(tzinfo=None) < recent_date:
                break
            if message.Class == 43 and message.Attachments.Count > 0:
                received_date = message.ReceivedTime.strftime("%Y-%m-%d")
                for i in range(1, message.Attachments.Count + 1):
                    attachment = message.Attachments.Item(i)
                    if attachment.FileName.lower().endswith(".pdf"):
                        filename = f"{received_date}_{attachment.FileName}"
                        filepath = os.path.join(save_dir, filename)
                        attachment.SaveAsFile(filepath)
                        saved_files.append((filepath, received_date))
        except Exception as e:
            logger.warning(f"âš ï¸ ë©”ì‹œì§€ ì˜¤ë¥˜: {e}")
    return saved_files



# PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì •ì œ
def extract_text_from_pdf(pdf_path):
    with fitz.open(pdf_path) as doc:
        return "\n".join(page.get_text() for page in doc)

def filter_relevant_text(text, min_length=30):
    lines = text.splitlines()
    clean_lines = []
    for line in lines:
        line = line.strip()
        if len(line) < min_length: continue
        if sum(c.isdigit() for c in line) > len(line) * 0.4: continue
        if re.search(r"ìˆ˜ìµë¥ |1D|3M|YTD|mailto:|@\w+|https?://", line, re.IGNORECASE): continue
        if re.match(r"^[-=â€¢â”‚\d\s]{3,}$", line): continue
        clean_lines.append(line)
    return "\n".join(dict.fromkeys(clean_lines))



# GPT ìš”ì•½ (requests ë°©ì‹)
def summarize_text(text):
    api_key = load_openai_key()
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}"
    }
    prompt = f"ê¸ˆìœµì‹œì¥ì´ë‚˜ ê²½ì œë¶„ì„ì— ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í•µì‹¬ ìš”ì  15ê°œë¡œ ìš”ì•½í•´ ì£¼ì„¸ìš”.\n\n{text[:2000]}"

    response = requests.post(
        "https://api.openai.com/v1/chat/completions",
        headers=headers,
        json={
            "model": "gpt-4",
            "messages": [
                {"role": "user", "content": prompt}
            ],
            "temperature": 0.3,
            "max_tokens": 1000
        },
        verify=False
    )

    if response.status_code == 200:
        return response.json()['choices'][0]['message']['content'].strip()
    else:
        return f"Error: {response.status_code}"

#? ==================================================================



#? ====================================================================
def generate_pdf_summary(days=30):
    import hashlib
    """
    ìµœê·¼ Outlook PDF ì²¨ë¶€íŒŒì¼ì„ ìˆ˜ì§‘í•˜ê³ , í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œ ë° ì •ì œí•œ í›„
    OpenAI GPTë¥¼ í†µí•´ ìš”ì•½í•˜ì—¬ PDF_summary ë¬¸ìì—´ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    1ì¼ ìºì‹œ ì‚¬ìš©.
    """
    try:
        # âœ… ìºì‹œ ë””ë ‰í† ë¦¬ ë° í‚¤ ìƒì„±
        CACHE_DIR = Path("cache/pdf_summary_cache")
        CACHE_DIR.mkdir(parents=True, exist_ok=True)
        cache_expiry = timedelta(days=1)

        # í‚¤ ìƒì„± (ê¸°ê°„ì´ ë‹¬ë¼ì§€ë©´ ë‹¤ë¥´ê²Œ ìºì‹±)
        cache_key = f"pdf_summary_{days}"
        cache_hash = hashlib.md5(cache_key.encode("utf-8")).hexdigest()
        cache_path = CACHE_DIR / f"{cache_hash}.txt"

        # âœ… ìºì‹œê°€ ìœ íš¨í•˜ë©´ ë°˜í™˜
        if cache_path.exists():
            mtime = datetime.fromtimestamp(cache_path.stat().st_mtime)
            if datetime.now() - mtime < cache_expiry:
                with open(cache_path, "r", encoding="utf-8") as f:
                    print("ğŸ” PDF ìš”ì•½ ìºì‹œ ì‚¬ìš©")
                    return f.read()

        # âœ… ìµœì‹  PDF ì²¨ë¶€íŒŒì¼ ê°€ì ¸ì˜¤ê¸°
        pdf_files = fetch_recent_pdf_attachments(days=days)
        if not pdf_files:
            return "ìµœê·¼ 30ì¼ê°„ ì €ì¥ëœ PDF ì²¨ë¶€íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."

        # âœ… í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì •ì œ
        pdf_texts = []
        for path, date in pdf_files:
            try:
                raw_text = extract_text_from_pdf(path)
                clean_text = filter_relevant_text(raw_text)
                if clean_text:
                    pdf_texts.append(clean_text)
            except Exception as e:
                logger.warning(f"âš ï¸ PDF ì²˜ë¦¬ ì˜¤ë¥˜ ({path}): {e}")

        combined_pdf_text = "\n\n".join(pdf_texts)

        # âœ… GPT ìš”ì•½
        if combined_pdf_text.strip():
            summary = summarize_text(combined_pdf_text)

            # âœ… ìºì‹œì— ì €ì¥
            with open(cache_path, "w", encoding="utf-8") as f:
                f.write(summary)

            print("âœ… PDF ìš”ì•½ ìºì‹œ ì €ì¥ë¨")
            return summary
        else:
            return "PDF í…ìŠ¤íŠ¸ê°€ ì¶©ë¶„í•˜ì§€ ì•Šì•„ ìš”ì•½í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    except Exception as e:
        logger.error(f"âŒ PDF ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")
        return f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
#? ====================================================================







#? <ë‰´ìŠ¤ìš”ì•½>============================================

def fetch_news_summary(query, start_date=None, end_date=None, page_size=5):
    import hashlib
    import requests
    import urllib3

    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

    # âœ… ìºì‹œ ì„¤ì •
    CACHE_DIR = Path("cache/news_cache")
    CACHE_DIR.mkdir(parents=True, exist_ok=True)
    cache_expiry = timedelta(days=1)

    # âœ… ìºì‹œ í‚¤ ìƒì„± (query + dates)
    key_string = f"{query}_{start_date}_{end_date}_{page_size}"
    key_hash = hashlib.md5(key_string.encode('utf-8')).hexdigest()
    cache_path = CACHE_DIR / f"{key_hash}.json"

    # âœ… ìºì‹œ ìœ íš¨ì„± ê²€ì‚¬
    if cache_path.exists():
        mtime = datetime.fromtimestamp(cache_path.stat().st_mtime)
        if datetime.now() - mtime < cache_expiry:
            with open(cache_path, "r", encoding="utf-8") as f:
                print(f"ğŸ” ë‰´ìŠ¤ ìºì‹œ ì‚¬ìš©: {query}")
                return json.load(f)

    # âœ… API í˜¸ì¶œ
    api_key = load_news_api_key()
    url = "https://newsapi.org/v2/everything"
    params = {
        "q": query,
        "language": "en",
        "sortBy": "publishedAt",
        "pageSize": page_size,
        "apiKey": api_key
    }

    if start_date:
        params["from"] = pd.to_datetime(start_date).strftime('%Y-%m-%d')
    if end_date:
        params["to"] = pd.to_datetime(end_date).strftime('%Y-%m-%d')

    try:
        res = requests.get(url, params=params, verify=False)
        res.raise_for_status()
    except requests.exceptions.RequestException as e:
        print(f"âŒ ë‰´ìŠ¤ ìš”ì²­ ì‹¤íŒ¨: {e}")
        return []

    articles = res.json().get("articles", [])
    summaries = [
        f"""- {a['title']} ({a['source']['name']})
  ğŸ“Œ {a.get('description', '').strip()}
  ğŸ” {a.get('content', '').split('â€¦')[0].strip()}"""
        for a in articles if a.get("description")
    ]

    # âœ… ìºì‹œ ì €ì¥
    with open(cache_path, "w", encoding="utf-8") as f:
        json.dump(summaries, f, ensure_ascii=False, indent=2)

    print(f"âœ… ë‰´ìŠ¤ ìºì‹œ ì €ì¥ë¨: {query}")
    return summaries
# ?==========================================================================



def get_news_digest(start_date=None, end_date=None, include_region=True, include_asset_class=True):
    topics = {}

    if include_region:
        topics.update({
            "ë¯¸êµ­": "US economy stock market FED inflation interest rate",
            "ìœ ëŸ½": "Europe ECB inflation interest rate",
            "ì¼ë³¸": "Japan JGB inflation interest rate",
            "ì´ë¨¸ì§•": "Emerging Stock Market",
        })

    if include_region or include_asset_class:
        topics["ê²½ì œì§€í‘œ"] = "economic indicators US Europe Japan Emerging"

    if include_asset_class:
        topics["ìì‚°êµ°"] = (
            "US Growth Stocks US Value Stocks US Treasury "
            "US Highyield US Real Estate Gold Oil "
            "Developed Market Emerging Market"
        )

    result = []
    for region, query in topics.items():
        summaries = fetch_news_summary(query, start_date=start_date, end_date=end_date)
        if summaries:
            result.append(f"[{region}]\n" + "\n".join(summaries))

    return "\n\n".join(result)



from datetime import datetime, timedelta

# ì˜ˆ: ì§€ë‚œ 30ì¼ ë‰´ìŠ¤ ìš”ì•½
today = datetime.today()
start = (today - timedelta(days=30)).strftime('%Y-%m-%d')
end = today.strftime('%Y-%m-%d')

news_summary = get_news_digest(start_date=start, end_date=end)
print("news_summary=====================\n", news_summary)











# GPT ê¸°ë°˜ ìš´ìš©ë³´ê³ ì„œ ìƒì„± í•¨ìˆ˜
def Run_GPT(nl_command, news_summary, PDF_summary, config_path, error_msg=None):
    try:
        with open(config_path, "r", encoding="utf-8") as f:
            api_key = yaml.safe_load(f)["openai_api_key"]
    except Exception as e:
        return f"âŒ CONFIG íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}", None

    headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}

    prompt = f"""
        - ì‚¬ìš©ì ëª…ë ¹: {nl_command}
        - news_summary : {news_summary}
        - PDF_summary : {PDF_summary}
        - ì‹œì¥í˜„í™©, ì‹œì¥ì „ë§, ìš´ìš©ê³„íš ë“±ì˜ ì „ì²´ ë‚´ìš© ì•½ 1500~2500ì ì´ë‚´ë¡œ ì½”ë©˜íŠ¸ ì‘ì„±í•´ì¤˜.
        - ê° í•­ëª©ì€ ë°˜ë“œì‹œ ëŒ€ê´„í˜¸ë¡œ êµ¬ë¶„ëœ ì œëª©ì„ í¬í•¨í•´ì¤˜ (ì˜ˆ: [ì‹œì¥í˜„í™©])
        - ì‚¬ìš©ìëª…ë ¹, news_summary, PDF_summaryë¥¼ ë°˜ì˜í•˜ì§€ë§Œ ì´ ì¶œì²˜ëŠ” ì–¸ê¸‰í•˜ì§€ë§ˆ.
        - news_summary, PDF_summaryëŠ” ê²½ì œì™€ ìì‚°êµ°ì— ëŒ€í•œ ì„¤ëª…ì— ë³´ì¡°ì  ì—­í• ì„ í•˜ëŠ” ìˆ˜ì¤€ìœ¼ë¡œë§Œ í™œìš©í•´ì£¼ê³  ì£¼ë¡œ ETFê´€ì ìœ¼ë¡œ ì„œìˆ í•´ì¤˜.
        - ê²½ì œì§€í‘œëŠ” êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ë¥¼ ì–¸ê¸‰í•´ì¤˜.
        - ìì‚°êµ°ê³¼ êµ­ê°€, region ì´ì™¸ì˜ ì£¼ì œ ì™¸ì—ëŠ” ëª¨ë‘ ë³´ì™„ì ìœ¼ë¡œë§Œ ì–¸ê¸‰í•´ì¤˜.
        - ì‹œì¥í˜„í™©ì— ì§€ì—­ë³„/ì„¹í„°ë³„ {ì›”ê°„ìˆ˜ìµë¥ }ì„ ì¼ë¶€ ì–¸ê¸‰í•´ì¤˜. 
        - **ë™ì¼ ì£¼ì œ ë°˜ë³µ ê¸ˆì§€**: ë™ì¼í•œ ì£¼ì œë‚˜ ë‚´ìš©ì´ ë°˜ë³µë˜ì§€ ì•Šë„ë¡ ì£¼ì˜í•´ì¤˜.
        - ê°™ì€ ë¬¸ì¥ ì–´ë¯¸ê°€ ë°˜ë³µë˜ì§€ ì•Šë„ë¡ ì–´ë¯¸ë¥¼ ë³€ê²½í•´ì¤˜.
        - **íˆ¬ìë¹„ì¤‘ ìˆ˜ì¹˜ ì–¸ê¸‰ ê¸ˆì§€**
        - ìš´ìš©ê³„íšì€ ìì‚°êµ°ê³¼ regionë³„ ê¸ì •ì  ë¶€ì •ì  ê²¬í•´ë¡œ ì„œìˆ í•´ì¤˜. 
        - ìˆ«ìëŠ” ì†Œìˆ˜ì  í•œ ìë¦¬ (ì˜ˆ: 12.1%)ë¡œ í‘œê¸°í•´ì¤˜.
    """

    if error_msg:
        missing = extract_missing_variable(error_msg)
        prompt += f"\n# ì˜¤ë¥˜:\n{error_msg}"
        if missing:
            prompt += f"\nğŸ’¡ '{missing}'ëŠ” ì •ì˜ë˜ì§€ ì•Šì€ ë³€ìˆ˜ì…ë‹ˆë‹¤. ì´ë¥¼ í¬í•¨í•´ ë‹¤ì‹œ ìƒì„±í•´ì£¼ì„¸ìš”."

    res = requests.post("https://api.openai.com/v1/chat/completions", headers=headers, json={
        "model": "gpt-4",
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0.3,
        "max_tokens": 2000
    }, verify=False)

    if res.status_code != 200:
        return f"# ì˜¤ë¥˜ ë°œìƒ: {res.status_code}\n{res.text}", prompt

    return res.json()['choices'][0]['message']['content'].strip(), prompt

# ì½œë°± ì •ì˜
@app.callback(
    Output('reply-output', 'children'),
    Input('run-button', 'n_clicks'),
    State('config-path', 'value'),
    State('nl-command', 'value'),
    prevent_initial_call=True
)
def update_output(n_clicks, config_path, nl_command):
    news_summary = get_news_digest(start_date=start, end_date=end)
    PDF_summary = generate_pdf_summary(days=30)
    reply, _ = Run_GPT(nl_command, news_summary, PDF_summary, config_path)
    return reply


app.clientside_callback(
    """
    function(n_clicks) {
        if (!n_clicks) return '';
        const text = document.getElementById('reply-output')?.innerText || '';
        if (text) {
            navigator.clipboard.writeText(text).then(function() {
                console.log("ğŸ“‹ ë³µì‚¬ ì™„ë£Œ");
            }).catch(function(err) {
                console.error("âŒ ë³µì‚¬ ì‹¤íŒ¨", err);
            });
        }
        return '';
    }
    """,
    Output('dummy-output', 'children'),  # ì•„ë¬´ ë°ë„ ì¶œë ¥í•˜ì§€ ì•ŠìŒ
    Input('copy-button', 'n_clicks'),
    prevent_initial_call=True
)



# ì„œë²„ ì‹¤í–‰
if __name__ == '__main__':
    app.run(debug=False, host='0.0.0.0', port=8050)

